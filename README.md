
# ğŸ¥” Potato Python Project (PotatoGPT)

A **local, private AI chatbot** built with **Streamlit + LangChain + RAG + Ollama**, fully containerized with **Docker**.

This project lets you:

* Chat with a local LLM (via Ollama)
* Ask questions about your own PDFs (RAG)
* Run everything reproducibly using Docker
* Persist embeddings across restarts

---

## âœ¨ Features

* ğŸ§  **Local LLM** using Ollama (no cloud, no API keys)
* ğŸ“„ **RAG (Retrieval-Augmented Generation)** over PDFs
* ğŸ’¬ **Conversation memory** (chat history aware)
* ğŸ–¥ **Streamlit UI**
* ğŸ³ **Dockerized** for easy setup
* ğŸ’¾ **Persistent ChromaDB**

---

## ğŸ—‚ Project Structure

```
potato-python-project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # Streamlit entrypoint
â”‚   â”œâ”€â”€ agent.py         # LangChain agent + memory
â”‚   â”œâ”€â”€ rag.py           # PDF loading + embeddings + Chroma
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ pdfs/         # ğŸ“„ Put your PDFs here
â”‚   â”‚   â””â”€â”€ chroma/       # ğŸ’¾ Persistent vector store
â”‚   â””â”€â”€ web/
â”‚       â””â”€â”€ Dockerfile    # App Dockerfile
â”œâ”€â”€ docker-compose.yml    # (optional) Compose setup
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tests/
â”œâ”€â”€ .github/workflows/    # CI (optional)
â””â”€â”€ README.md
```

---

## ğŸ§  Architecture Overview

```
Browser
  â†“ (8501)
Streamlit UI (main.py)
  â†“
LangChain Agent (agent.py)
  â†“
Retriever (rag.py)
  â†“
ChromaDB (embeddings)
  â†“
Ollama (LLM + Embeddings)
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Install Ollama (host machine)

ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

Then pull required models:

```bash
ollama pull llama3
ollama pull nomic-embed-text
```

Make sure Ollama is running:

```bash
ollama list
```

---

### 2ï¸âƒ£ Add your PDFs

Put at least **one PDF** into:

```
app/data/pdfs/
```

---

### 3ï¸âƒ£ Build Docker Image

From project root:

```bash
docker build --no-cache -t potatogpt .
```

---

### 4ï¸âƒ£ Run the App

```bash
docker run -p 8501:8501 \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  --name potatogpt-app \
  potatogpt
```

Open your browser:

ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

---

## ğŸ³ Docker Compose (Recommended)

Create `docker-compose.yml`:

```yaml
version: "3.9"
services:
  app:
    image: potatogpt
    container_name: potatogpt-app
    ports:
      - "8501:8501"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
      - ./app/data/pdfs:/app/data/pdfs
      - ./app/data/chroma:/app/data/chroma
    restart: unless-stopped
```

Run:

```bash
docker compose up --build
```

---

## ğŸ”„ Autoâ€‘Reload PDFs (No Rebuild Needed)

* PDFs are **mounted as volumes**
* Add / replace PDFs in `app/data/pdfs`
* Restart container:

```bash
docker restart potatogpt-app
```

Embeddings are reused if Chroma already exists.

---

## ğŸ’¾ Persistent ChromaDB

Chroma is stored at:

```
app/data/chroma/
```

Because it is mounted as a volume:

* Restarting Docker **does NOT delete embeddings**
* Rebuilding the image keeps your vector DB

---

## ğŸ§ª Development (Optional)

Run locally without Docker:

```bash
pip install -r requirements.txt
streamlit run app/main.py
```

Set Ollama URL if needed:

```bash
export OLLAMA_BASE_URL=http://localhost:11434
```

---

## â“ Common Issues

### âŒ Ollama connection error

Make sure:

* Ollama is running on host
* `OLLAMA_BASE_URL` is set correctly

For Docker on Windows/macOS:

```
http://host.docker.internal:11434
```

---

### âŒ No PDFs found

Ensure:

```
app/data/pdfs/*.pdf
```

At least one valid PDF is required.

---

## ğŸ” Privacy

* No cloud APIs
* No data leaves your machine
* Fully offline after model download

---

## ğŸ“Œ Tech Stack

* Python 3.11
* Streamlit
* LangChain
* ChromaDB
* Ollama
* Docker

---

## ğŸ§­ Roadmap

* [ ] GPU support
* [ ] Auth & multi-user
* [ ] PDF upload via UI
* [ ] LangGraph workflows
* [ ] Cloud deployment templates

---

## ğŸ¥” Final Words

This project is a **production-grade local AI system**, not a toy.

You now have:

* Real RAG
* Real containers
* Real AI infra skills
